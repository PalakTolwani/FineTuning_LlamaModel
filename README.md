LLaMA-2-7B Fine-Tuning with Low VRAM Support

Description:
This repository provides a complete setup for fine-tuning LLaMA-2 models (7B parameters) on instruction datasets using PEFT (LoRA), 4-bit quantization, and gradient checkpointing. It is optimized for low VRAM environments like Google Colab (4â€“16 GB GPU) and includes:

Preprocessing and tokenization of instruction datasets

Memory-efficient model loading with device offloading

Training using SFTTrainer from Hugging Face TRL

Integration with BitsAndBytes 4-bit quantization to drastically reduce GPU memory usage

Gradient checkpointing and LoRA for lightweight parameter updates

Ready-to-use scripts for training and saving fine-tuned models

Key Features:

ðŸ’¾ Low VRAM friendly

âš¡ Fast fine-tuning with LoRA and 4-bit quantization

ðŸ”„ Full compatibility with Hugging Face Transformers, PEFT, and TRL

ðŸ“‚ Easy deployment: models can be pushed to Hugging Face Hub for sharing or inference

Usage:

Install dependencies with the provided one-shot Colab cell

Load your dataset or use the default instruction dataset

Run fine-tuning scripts with SFTTrainer

Save or push your fine-tuned model to Hugging Face Hub
